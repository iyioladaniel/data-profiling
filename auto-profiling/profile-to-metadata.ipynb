{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ydata_profiling import ProfileReport\n",
    "from ydata_profiling.config import Settings\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "import getpass\n",
    "import oracledb\n",
    "from clickhouse_driver import Client # Import the clickhouse driver\n",
    "from dotenv import load_dotenv # Import load_dotenv to load environment variables from .env file\n",
    "#from .autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88537ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def connect_to_oracle(db_user: str, db_password: str, db_host: str, db_port, db_sid) -> None:\n",
    "    try: \n",
    "        with oracledb.connect(user=db_user, password=db_password, dsn=oracledb.makedsn(db_host, db_port, db_sid)) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                sql = \"\"\"select sysdate from dual\"\"\"\n",
    "                for r in cursor.execute(sql):\n",
    "                    print(r)\n",
    "    except oracledb.DatabaseError as e:\n",
    "        error, = e.args\n",
    "        print(f\"Oracle error code: {error.code}\")\n",
    "        print(f\"Oracle error message: {error.message}\")\n",
    "    return None\n",
    "'''\n",
    "''' --- Configuration Paths ---\n",
    "Calculate the path to the secrets file relative to the script's directory\n",
    "'''\n",
    "# Calculate the path to the table list file relative to the project root\n",
    "script_dir = os.path.dirname(__file__)\n",
    "project_root = os.path.join(script_dir, '..')\n",
    "\n",
    "tables_path= os.path.join(project_root, 'scripts', 'flex11_table_list.txt')\n",
    "\n",
    "# Calculate the path to the output directory relative to the project root\n",
    "metadata_dir= os.path.join(project_root, 'metadata_profile')\n",
    "\n",
    "# Load environment variables from the specified secrets file\n",
    "# We use override=True to ensure variables in this file take precedence if they exist elsewhere\n",
    "secrets_path = os.path.join(project_root, 'env', 'secrets.env')\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "# We use override=True to ensure variables in this file take precedence if they exist elsewhere\n",
    "load_dotenv(dotenv_path=secrets_path, override=True)\n",
    "\n",
    "\n",
    "def connect_to_clickhouse(user: str, password: str, host: str, port: int) -> None:\n",
    "    '''\n",
    "    Connect to Clickhouse database.\n",
    "\n",
    "    Args:\n",
    "        db_user (str): Database username\n",
    "        db_password (str): Database password\n",
    "        db_host (str): Database host\n",
    "        db_port (int): Database port\n",
    "\n",
    "    Returns:\n",
    "        clickhouse_driver.Client: ClickHouse client connection object, or None if connection fails\n",
    "    '''\n",
    "    try:\n",
    "        # Use the Client object for connection\n",
    "        client = Client(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        # Test the connection by executing a simple query\n",
    "        client.execute('SELECT 1')\n",
    "        print(f\"Successfully connected to ClickHouse at {host}:{port}\")\n",
    "        return client # Return the client object on success\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to ClickHouse: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_list_of_tables(path_to_file: str) -> list:\n",
    "    \"\"\"\n",
    "    Get a list of tables from a file.\n",
    "    Args:\n",
    "        path_to_file (str): path to file containing table names\n",
    "        \n",
    "    Returns:\n",
    "        list: List of table names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_file, 'r') as file:\n",
    "            tables = [line.strip() for line in file.readlines()]\n",
    "        return tables\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at the path '{path_to_file}'. Please check the path and try again.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def hash_column(column: pd.Series) -> pd.Series:\n",
    "    \"\"\"Hash the values in a column using SHA-256.\"\"\"\n",
    "    return column.apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest() if pd.notnull(x) else x)\n",
    "\n",
    "def generate_profiling_report(db_connection=Client, tables_list=None, path_to_csv=None, \n",
    "                             sensitive_columns=None, sensitive_keywords=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a profiling report using ydata-profiling for Clickhouse database tables\n",
    "    \n",
    "    Args:\n",
    "        db_connection (clickhouse_driver.Client, optional): Connection to Clickhouse database\n",
    "        schema (str, optional): Schema name for clickhouse tables- multiple schema does not exist\n",
    "        tables_list (list, optional): List of tables to profile\n",
    "        sensitive_columns (list, optional): List of column names to mark as sensitive\n",
    "        sensitive_keywords (list, optional): Keywords to detect sensitive columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with profiling information including completeness metrics\n",
    "    \"\"\"\n",
    "    # Default sensitive keywords if not provided\n",
    "    if sensitive_keywords is None:\n",
    "        sensitive_keywords = [\"bvn\", \"id number\", \"nin\", \"passport\", \"driver\", \n",
    "                             \"identificationnumber\", \"chn\"]\n",
    "    \n",
    "    results_dfs = []  # To store results from multiple tables\n",
    "    \n",
    "    try:\n",
    "        if db_connection and tables_list:\n",
    "            # Process ClickHouse tables\n",
    "            for table_name in tables_list: # Iterate directly over table names\n",
    "                try:\n",
    "                    full_table_name = table_name\n",
    "                    db_name = db_connection.database # CHANGED: Get the database name from the client object (will be default if not specified) -> default\n",
    "                    \n",
    "                    # Check if the table exists in the database\n",
    "                    print(f\"Processing table: {full_table_name}\")\n",
    "\n",
    "                    # Construct the SQL query for ClickHouse\n",
    "                    # This query uses the table_name, relying on the default database connection\n",
    "                    query = f\"SELECT * FROM {table_name}\" # CHANGED: Use table_name directly in query\n",
    "\n",
    "                    # execute returns a list of tuples, need to get column names separately\n",
    "                    data_tuples = db_connection.execute(query)\n",
    "\n",
    "                    # Get column names from the table description using client.execute()\n",
    "                    column_names = [col[0] for col in db_connection.execute(f\"DESCRIBE TABLE {table_name}\")] # CHANGED: Use table_name in DESCRIBE TABLE query\n",
    "\n",
    "                    # Create pandas DataFrame\n",
    "                    data = pd.DataFrame(data_tuples, columns=column_names)\n",
    "\n",
    "\n",
    "                    # For empty tables\n",
    "                    if data.empty:\n",
    "                        print(f\"Table {full_table_name} is empty. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Pass the database name obtained from the client as schema_name\n",
    "                    result_df = _process_dataset(data, full_table_name, table_name, db_name, # CHANGED: Pass db_name\n",
    "                                               sensitive_columns, sensitive_keywords)\n",
    "                    results_dfs.append(result_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table {full_table_name}: {e}\")\n",
    "        else: # This else block now covers cases where db_connection or tables_list are missing\n",
    "            print(\"Error: db_connection and tables_list must be provided for database profiling.\") # Updated error message\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Combine all results\n",
    "        if results_dfs:\n",
    "            # Use ignore_index=True to reset index when concatenating\n",
    "            combined_df = pd.concat(results_dfs, ignore_index=True)\n",
    "            return combined_df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def _process_dataset(data, source_name, table_name, schema_name, sensitive_columns, sensitive_keywords):\n",
    "    \"\"\"Helper function to process a DB table\"\"\"\n",
    "    # Store the total record count\n",
    "    total_records = len(data)\n",
    "    \n",
    "    # Automatically detect sensitive columns if not provided\n",
    "    if sensitive_columns is None:\n",
    "        sensitive_columns = [\n",
    "            col for col in data.columns \n",
    "            if any(keyword in col.lower() for keyword in sensitive_keywords)\n",
    "        ]\n",
    "    \n",
    "    # Hash sensitive columns\n",
    "    if sensitive_columns:\n",
    "        for col in sensitive_columns:\n",
    "            if col in data.columns:\n",
    "                print(f\"Hashing sensitive column: {col}\")\n",
    "                data[col] = hash_column(data[col])\n",
    "    \n",
    "    # Configure settings to mark sensitive columns\n",
    "    config = Settings()\n",
    "    if sensitive_columns:\n",
    "        config.variables.descriptions = {col: \"Sensitive Data (Hashed)\" \n",
    "                                       for col in sensitive_columns \n",
    "                                       if col in data.columns}\n",
    "    \n",
    "    # Generate profiling report    \n",
    "    profile = ProfileReport(\n",
    "        data,\n",
    "        title=f\"{source_name} Profiling Report\",\n",
    "        explorative=True,\n",
    "        config=config)\n",
    "    \n",
    "    # Get JSON data and extract variables data\n",
    "    json_data = profile.to_json()\n",
    "    variables_data = json.loads(json_data)['variables']\n",
    "    variables_df = pd.DataFrame(variables_data).transpose()\n",
    "    variables_df = variables_df.reset_index().rename(columns={'index': 'column_name'})\n",
    "    \n",
    "    # Add metadata enrichment\n",
    "    variables_df['table_name'] = table_name\n",
    "    variables_df['schema_name'] = schema_name # this will be the \"default\" database name\n",
    "    variables_df['total_records'] = total_records\n",
    "    variables_df['created_at'] = pd.Timestamp.now()\n",
    "    variables_df['last_updated'] = pd.Timestamp.now()\n",
    "    \n",
    "    # Calculate completeness percentage\n",
    "    if 'count' in variables_df.columns and 'n_missing' in variables_df.columns:\n",
    "        variables_df['completeness_pct'] = ((variables_df['count'] - variables_df['n_missing']) / \n",
    "                                         variables_df['count'] * 100).round(2)\n",
    "    \n",
    "    # Mark sensitive columns in the metadata\n",
    "    variables_df['is_sensitive'] = variables_df['column_name'].isin(sensitive_columns)\n",
    "    \n",
    "    print(f\"Successfully generated profile for {source_name} with {len(variables_df)} columns\")\n",
    "    \n",
    "    return variables_df\n",
    "\n",
    "def generate_metadata_file(var_df: pd.DataFrame, output_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a metadata file from the profiling report.\n",
    "    If the file exists, checks for duplicates and appends only new records.\n",
    "    \n",
    "    Args:\n",
    "        var_df (DataFrame): DataFrame with profiling information\n",
    "        output_path (str, optional): Path to save the metadata file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The processed metadata DataFrame that was saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        metadata_df = var_df.copy()\n",
    "        \n",
    "        # Set default filename based on schema and table if available\n",
    "       \n",
    "        # Determine the filename based on table name\n",
    "        # Ensure metadata_df is not empty and has the required columns\n",
    "        if metadata_df.empty or 'table_name' not in metadata_df.columns:\n",
    "             print(\"Error: Metadata DataFrame is empty or missing required column ('table_name'). Cannot determine filename.\")\n",
    "             return pd.DataFrame() # Return empty DataFrame if essential info is missing\n",
    "\n",
    "        # Use first row's table (assuming all rows are for same table for a single file)\n",
    "        table = metadata_df['table_name'].iloc[0]\n",
    "        if not table:\n",
    "             print(\"Error: Table name is missing in the metadata. Cannot determine filename.\")\n",
    "             return pd.DataFrame()\n",
    "\n",
    "        # Construct the full output file path within the output directory\n",
    "        output_filename = f\"{table}_metadata.csv\"\n",
    "        output_path = os.path.join(metadata_dir, output_filename) # CHANGED: Construct full path using output_dir\n",
    "\n",
    "        \n",
    "        # Rename columns if they have their original names\n",
    "        rename_map = {\n",
    "            'n_distinct': 'distinct_count',\n",
    "            'p_distinct': 'distinct_percentage',\n",
    "            'is_unique': 'is_unique',\n",
    "            'type': 'data_type',\n",
    "            'n_unique': 'unique_count',\n",
    "            'p_unique': 'unique_percentage',\n",
    "            'n_missing': 'missing_count',\n",
    "            'n': 'total_count',\n",
    "            'p_missing': 'missing_percentage',\n",
    "            'n_category': 'category_count'\n",
    "        }\n",
    "        \n",
    "        # Only rename columns that exist and haven't been renamed yet\n",
    "        rename_cols = {k: v for k, v in rename_map.items() if k in metadata_df.columns and v not in metadata_df.columns}\n",
    "        if rename_cols:\n",
    "            metadata_df.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        # Ensure these columns are included at the beginning\n",
    "        priority_cols = ['schema_name', 'table_name', 'column_name', 'data_type', \n",
    "                        'total_records', 'total_count', 'missing_count', \n",
    "                        'completeness_pct', 'is_sensitive',\n",
    "                        'created_at', 'last_updated']\n",
    "        \n",
    "        # Create a list of all columns with priority columns first\n",
    "        all_cols = []\n",
    "        for col in priority_cols:\n",
    "            if col in metadata_df.columns:\n",
    "                all_cols.append(col)\n",
    "                \n",
    "        # Add remaining columns\n",
    "        for col in metadata_df.columns:\n",
    "            if col not in all_cols:\n",
    "                all_cols.append(col)\n",
    "                \n",
    "        # Reorder columns\n",
    "        metadata_df = metadata_df[all_cols]\n",
    "        \n",
    "        # Check if file exists and handle append logic\n",
    "        if os.path.exists(output_path):\n",
    "            # Read existing metadata\n",
    "            existing_df = pd.read_csv(output_path)\n",
    "            \n",
    "            # Define key columns for identifying duplicates\n",
    "            # A row is considered a duplicate if schema, table, and column name match\n",
    "            key_columns = ['schema_name', 'table_name', 'column_name']\n",
    "            key_columns = [col for col in key_columns if col in metadata_df.columns and col in existing_df.columns]\n",
    "            \n",
    "            if key_columns:  # Only proceed with duplicate check if we have key columns\n",
    "                # Filter out rows that already exist in the file\n",
    "                # Create a set of tuples with the key values from existing data\n",
    "                existing_keys = set(\n",
    "                    tuple(row) for row in existing_df[key_columns].itertuples(index=False, name=None)\n",
    "                )\n",
    "                \n",
    "                # Filter new data to only include rows with new keys\n",
    "                new_data_mask = ~metadata_df.apply(\n",
    "                    lambda row: tuple(row[key_columns]) in existing_keys, axis=1\n",
    "                )\n",
    "                \n",
    "                # If we have any new data, append it\n",
    "                if new_data_mask.any():\n",
    "                    metadata_df = metadata_df[new_data_mask]\n",
    "                    # Append new data to existing file\n",
    "                    metadata_df.to_csv(output_path, mode='a', header=False, index=False)\n",
    "                    print(f\"Appended {new_data_mask.sum()} new records to {output_path}\")\n",
    "                else:\n",
    "                    print(f\"No new records to append to {output_path}\")\n",
    "                    \n",
    "                # Combine for return value\n",
    "                metadata_df = pd.concat([existing_df, metadata_df])\n",
    "            else:\n",
    "                # If we can't determine duplicates, append all (might cause duplicates)\n",
    "                metadata_df.to_csv(output_path, mode='a', header=False, index=False)\n",
    "                print(f\"Appended all records to {output_path} (duplicate checking unavailable)\")\n",
    "                \n",
    "        else:\n",
    "            # File doesn't exist, create new\n",
    "            metadata_df.to_csv(output_path, index=False)\n",
    "            print(f\"Created new metadata file at {output_path}\")\n",
    "        \n",
    "        return metadata_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the metadata file: {e}\")\n",
    "        return var_df  # Return original DataFrame if we encounter an error\n",
    "    \n",
    "\n",
    "    # Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # --- ClickHouse Connection Details ---\n",
    "    # Read credentials from environment variables loaded from the secrets file\n",
    "    ch_host = os.getenv('host')\n",
    "    ch_port = int(os.getenv('port'))\n",
    "    ch_user = os.getenv('user')\n",
    "    ch_password = os.getenv('password')\n",
    "\n",
    "    # --- Paths to the file containing table names and the output directory ---\n",
    "    # These are now defined above using os.path.join relative to the project root\n",
    "\n",
    "    # --- Sensitive Column Configuration ---\n",
    "    # Optional: List specific column names to mark as sensitive, overrides keyword detection\n",
    "    # Read from env var, assuming comma-separated string\n",
    "    # SPECIFIC_SENSITIVE_COLUMNS_STR = os.getenv('SPECIFIC_SENSITIVE_COLUMNS')\n",
    "    # SPECIFIC_SENSITIVE_COLUMNS = [col.strip() for col in SPECIFIC_SENSITIVE_COLUMNS_STR.split(',')] if SPECIFIC_SENSITIVE_COLUMNS_STR else None\n",
    "\n",
    "    # Optional: Keywords to detect sensitive columns automatically if SPECIFIC_SENSITIVE_COLUMNS is None\n",
    "    sensitive_keywords_list = [\"bvn\", \"id number\", \"nin\", \"passport\", \"driver\",\n",
    "                             \"identificationnumber\", \"chn\", \"email\", \"phone\"]\n",
    "\n",
    "\n",
    "    # Validate essential configuration\n",
    "    if not all([ch_host, ch_user, ch_password]):\n",
    "        print(\"Error: Essential ClickHouse connection details (CH_HOST, CH_USER, CH_PASSWORD) not found in environment variables loaded from the secrets file.\")\n",
    "        print(f\"CH_HOST: {ch_host}, CH_PORT: {ch_port}, CH_USER: {ch_user}\")\n",
    "        exit(1) # Exit if essential connection details are missing\n",
    "\n",
    "    # Check if the calculated paths exist\n",
    "    if not os.path.exists(tables_path):\n",
    "        print(f\"Error: Calculated table list file path does not exist: {tables_path}\")\n",
    "        exit(1)\n",
    "\n",
    "    # The output directory will be created by generate_metadata_file, so no need to check existence here.\n",
    "    # However, we should ensure METADATA_OUTPUT_DIR is not None or empty if it wasn't calculated correctly.\n",
    "    if not os.path.exists(metadata_dir):\n",
    "        print(f\"Error: Calculated table list file path does not exist: {metadata_dir}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "    # 2. Connect to ClickHouse\n",
    "    ch_client = connect_to_clickhouse(ch_host, ch_port, ch_user, ch_password)\n",
    "\n",
    "    if ch_client:\n",
    "        # 3. Get list of tables from the file\n",
    "        tables_to_profile = get_list_of_tables(tables_path) # Use the calculated path\n",
    "\n",
    "        if tables_to_profile:\n",
    "            # 4. Generate profiling report for ClickHouse tables\n",
    "            profiling_results_df = generate_profiling_report(\n",
    "                db_connection=ch_client,\n",
    "                tables_list=tables_to_profile,\n",
    "                # sensitive_columns=SPECIFIC_SENSITIVE_COLUMNS, # Use the list from env var or None\n",
    "                sensitive_keywords=sensitive_keywords_list\n",
    "            )\n",
    "\n",
    "            # 5. Generate and save the metadata file to the specified directory\n",
    "            if not profiling_results_df.empty:\n",
    "                generate_metadata_file(profiling_results_df, output_dir=(metadata_dir)) # Use the calculated directory\n",
    "            else:\n",
    "                print(\"No profiling results to generate metadata file.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No tables found in the specified file to profile.\")\n",
    "\n",
    "        # Close the ClickHouse connection\n",
    "        ch_client.close()\n",
    "    else:\n",
    "        print(\"Failed to connect to ClickHouse. Cannot proceed with profiling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
