{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ydata_profiling import ProfileReport\n",
    "from ydata_profiling.config import Settings\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "import getpass\n",
    "import oracledb\n",
    "from dotenv import load_dotenv\n",
    "#from .autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88537ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_oracle(db_user: str, db_password: str, db_host: str, db_port, db_sid) -> None:\n",
    "    try: \n",
    "        with oracledb.connect(user=db_user, password=db_password, dsn=oracledb.makedsn(db_host, db_port, db_sid)) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                sql = \"\"\"select sysdate from dual\"\"\"\n",
    "                for r in cursor.execute(sql):\n",
    "                    print(r)\n",
    "    except oracledb.DatabaseError as e:\n",
    "        error, = e.args\n",
    "        print(f\"Oracle error code: {error.code}\")\n",
    "        print(f\"Oracle error message: {error.message}\")\n",
    "    return None\n",
    "\n",
    "def get_list_of_tables(path_to_file: str) -> list:\n",
    "    \"\"\"\n",
    "    Get a list of tables from a file.\n",
    "    \n",
    "    Args:\n",
    "        path_to_file (str): path to file containing table names\n",
    "        \n",
    "    Returns:\n",
    "        list: List of table names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_file, 'r') as file:\n",
    "            tables = [line.strip() for line in file.readlines()]\n",
    "        return tables\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at the path '{path_to_file}'. Please check the path and try again.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def hash_column(column: pd.Series) -> pd.Series:\n",
    "    \"\"\"Hash the values in a column using SHA-256.\"\"\"\n",
    "    return column.apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest() if pd.notnull(x) else x)\n",
    "\n",
    "def generate_profiling_report(db_connection=None, schema=None, tables_list=None, path_to_csv=None, \n",
    "                             sensitive_columns=None, sensitive_keywords=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a profiling report using ydata-profiling for either CSV files or Oracle database tables\n",
    "    \n",
    "    Args:\n",
    "        db_connection (oracledb.Connection, optional): Connection to Oracle database\n",
    "        schema (str, optional): Schema name for Oracle tables\n",
    "        tables_list (list, optional): List of tables to profile\n",
    "        path_to_csv (str, optional): Path to CSV file (if not using database)\n",
    "        sensitive_columns (list, optional): List of column names to mark as sensitive\n",
    "        sensitive_keywords (list, optional): Keywords to detect sensitive columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with profiling information including completeness metrics\n",
    "    \"\"\"\n",
    "    # Default sensitive keywords if not provided\n",
    "    if sensitive_keywords is None:\n",
    "        sensitive_keywords = [\"bvn\", \"id number\", \"nin\", \"passport\", \"driver\", \n",
    "                             \"identificationnumber\", \"chn\"]\n",
    "    \n",
    "    results_dfs = []  # To store results from multiple tables\n",
    "    \n",
    "    try:\n",
    "        # Determine source type (CSV or database)\n",
    "        if path_to_csv:\n",
    "            # Process CSV file\n",
    "            file_name = str.split(path_to_csv, \"/\")[-1].split(\".\")[0]\n",
    "            data = pd.read_csv(path_to_csv)\n",
    "            source_name = file_name\n",
    "            table_name = file_name\n",
    "            schema_name = None\n",
    "            \n",
    "            # Process single CSV\n",
    "            result_df = _process_dataset(data, source_name, table_name, schema_name, \n",
    "                                         sensitive_columns, sensitive_keywords)\n",
    "            results_dfs.append(result_df)\n",
    "            \n",
    "        elif db_connection and tables_list:\n",
    "            # Process Oracle tables\n",
    "            for table in tables_list:\n",
    "                try:\n",
    "                    print(f\"Processing table: {schema}.{table}\")\n",
    "                    query = f\"SELECT * FROM {schema}.{table}\"\n",
    "                    data = pd.read_sql(query, db_connection)\n",
    "                    \n",
    "                    # For empty tables\n",
    "                    if data.empty:\n",
    "                        print(f\"Table {schema}.{table} is empty. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    result_df = _process_dataset(data, f\"{schema}.{table}\", table, schema,\n",
    "                                               sensitive_columns, sensitive_keywords)\n",
    "                    results_dfs.append(result_df)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing table {schema}.{table}: {e}\")\n",
    "        else:\n",
    "            print(\"Error: Either provide path_to_csv or both db_connection and tables_list\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine all results\n",
    "        if results_dfs:\n",
    "            combined_df = pd.concat(results_dfs)\n",
    "            return combined_df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _process_dataset(data, source_name, table_name, schema_name, sensitive_columns, sensitive_keywords):\n",
    "    \"\"\"Helper function to process a single dataset (CSV or DB table)\"\"\"\n",
    "    # Store the total record count\n",
    "    total_records = len(data)\n",
    "    \n",
    "    # Automatically detect sensitive columns if not provided\n",
    "    if sensitive_columns is None:\n",
    "        sensitive_columns = [\n",
    "            col for col in data.columns \n",
    "            if any(keyword in col.lower() for keyword in sensitive_keywords)\n",
    "        ]\n",
    "    \n",
    "    # Hash sensitive columns\n",
    "    if sensitive_columns:\n",
    "        for col in sensitive_columns:\n",
    "            if col in data.columns:\n",
    "                print(f\"Hashing sensitive column: {col}\")\n",
    "                data[col] = hash_column(data[col])\n",
    "    \n",
    "    # Configure settings to mark sensitive columns\n",
    "    config = Settings()\n",
    "    if sensitive_columns:\n",
    "        config.variables.descriptions = {col: \"Sensitive Data (Hashed)\" \n",
    "                                       for col in sensitive_columns \n",
    "                                       if col in data.columns}\n",
    "    \n",
    "    # Generate profiling report    \n",
    "    profile = ProfileReport(\n",
    "        data,\n",
    "        title=f\"{source_name} Profiling Report\",\n",
    "        explorative=True,\n",
    "        config=config)\n",
    "    \n",
    "    # Get JSON data and extract variables data\n",
    "    json_data = profile.to_json()\n",
    "    variables_data = json.loads(json_data)['variables']\n",
    "    variables_df = pd.DataFrame(variables_data).transpose()\n",
    "    variables_df = variables_df.reset_index().rename(columns={'index': 'column_name'})\n",
    "    \n",
    "    # Add metadata enrichment\n",
    "    variables_df['table_name'] = table_name\n",
    "    variables_df['schema_name'] = schema_name\n",
    "    variables_df['total_records'] = total_records\n",
    "    variables_df['created_at'] = pd.Timestamp.now()\n",
    "    variables_df['last_updated'] = pd.Timestamp.now()\n",
    "    \n",
    "    # Calculate completeness percentage\n",
    "    if 'count' in variables_df.columns and 'n_missing' in variables_df.columns:\n",
    "        variables_df['completeness_pct'] = ((variables_df['count'] - variables_df['n_missing']) / \n",
    "                                         variables_df['count'] * 100).round(2)\n",
    "    \n",
    "    # Mark sensitive columns in the metadata\n",
    "    variables_df['is_sensitive'] = variables_df['column_name'].isin(sensitive_columns)\n",
    "    \n",
    "    print(f\"Successfully generated profile for {source_name} with {len(variables_df)} columns\")\n",
    "    \n",
    "    return variables_df\n",
    "\n",
    "def generate_metadata_file(var_df: pd.DataFrame, output_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a metadata file from the profiling report.\n",
    "    If the file exists, checks for duplicates and appends only new records.\n",
    "    \n",
    "    Args:\n",
    "        var_df (DataFrame): DataFrame with profiling information\n",
    "        output_path (str, optional): Path to save the metadata file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The processed metadata DataFrame that was saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        metadata_df = var_df.copy()\n",
    "        \n",
    "        # Set default filename based on schema and table if available\n",
    "        if output_path is None:\n",
    "            if 'schema_name' in metadata_df.columns and 'table_name' in metadata_df.columns:\n",
    "                # Use first row's schema and table (assuming all rows are for same table)\n",
    "                schema = metadata_df['schema_name'].iloc[0]\n",
    "                table = metadata_df['table_name'].iloc[0]\n",
    "                if schema and table:\n",
    "                    output_path = f\"{schema}_{table}_metadata.csv\"\n",
    "                else:\n",
    "                    output_path = \"metadata_report.csv\"\n",
    "            else:\n",
    "                output_path = \"metadata_report.csv\"\n",
    "        \n",
    "        # Rename columns if they have their original names\n",
    "        rename_map = {\n",
    "            'n_distinct': 'distinct_count',\n",
    "            'p_distinct': 'distinct_percentage',\n",
    "            'is_unique': 'is_unique',\n",
    "            'type': 'data_type',\n",
    "            'n_unique': 'unique_count',\n",
    "            'p_unique': 'unique_percentage',\n",
    "            'n_missing': 'missing_count',\n",
    "            'n': 'total_count',\n",
    "            'p_missing': 'missing_percentage',\n",
    "            'n_category': 'category_count'\n",
    "        }\n",
    "        \n",
    "        # Only rename columns that exist and haven't been renamed yet\n",
    "        rename_cols = {k: v for k, v in rename_map.items() if k in metadata_df.columns and v not in metadata_df.columns}\n",
    "        if rename_cols:\n",
    "            metadata_df.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        # Ensure these columns are included at the beginning\n",
    "        priority_cols = ['schema_name', 'table_name', 'column_name', 'data_type', \n",
    "                        'total_records', 'total_count', 'missing_count', \n",
    "                        'completeness_pct', 'is_sensitive',\n",
    "                        'created_at', 'last_updated']\n",
    "        \n",
    "        # Create a list of all columns with priority columns first\n",
    "        all_cols = []\n",
    "        for col in priority_cols:\n",
    "            if col in metadata_df.columns:\n",
    "                all_cols.append(col)\n",
    "                \n",
    "        # Add remaining columns\n",
    "        for col in metadata_df.columns:\n",
    "            if col not in all_cols:\n",
    "                all_cols.append(col)\n",
    "                \n",
    "        # Reorder columns\n",
    "        metadata_df = metadata_df[all_cols]\n",
    "        \n",
    "        # Check if file exists and handle append logic\n",
    "        if os.path.exists(output_path):\n",
    "            # Read existing metadata\n",
    "            existing_df = pd.read_csv(output_path)\n",
    "            \n",
    "            # Define key columns for identifying duplicates\n",
    "            # A row is considered a duplicate if schema, table, and column name match\n",
    "            key_columns = ['schema_name', 'table_name', 'column_name']\n",
    "            key_columns = [col for col in key_columns if col in metadata_df.columns and col in existing_df.columns]\n",
    "            \n",
    "            if key_columns:  # Only proceed with duplicate check if we have key columns\n",
    "                # Filter out rows that already exist in the file\n",
    "                # Create a set of tuples with the key values from existing data\n",
    "                existing_keys = set(\n",
    "                    tuple(row) for row in existing_df[key_columns].itertuples(index=False, name=None)\n",
    "                )\n",
    "                \n",
    "                # Filter new data to only include rows with new keys\n",
    "                new_data_mask = ~metadata_df.apply(\n",
    "                    lambda row: tuple(row[key_columns]) in existing_keys, axis=1\n",
    "                )\n",
    "                \n",
    "                # If we have any new data, append it\n",
    "                if new_data_mask.any():\n",
    "                    metadata_df = metadata_df[new_data_mask]\n",
    "                    # Append new data to existing file\n",
    "                    metadata_df.to_csv(output_path, mode='a', header=False, index=False)\n",
    "                    print(f\"Appended {new_data_mask.sum()} new records to {output_path}\")\n",
    "                else:\n",
    "                    print(f\"No new records to append to {output_path}\")\n",
    "                    \n",
    "                # Combine for return value\n",
    "                metadata_df = pd.concat([existing_df, metadata_df])\n",
    "            else:\n",
    "                # If we can't determine duplicates, append all (might cause duplicates)\n",
    "                metadata_df.to_csv(output_path, mode='a', header=False, index=False)\n",
    "                print(f\"Appended all records to {output_path} (duplicate checking unavailable)\")\n",
    "                \n",
    "        else:\n",
    "            # File doesn't exist, create new\n",
    "            metadata_df.to_csv(output_path, index=False)\n",
    "            print(f\"Created new metadata file at {output_path}\")\n",
    "        \n",
    "        return metadata_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the metadata file: {e}\")\n",
    "        return var_df  # Return original DataFrame if we encounter an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1442fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = \"../data/dummy/dummy-data/asset_custdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0edbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file name from the argument passed\n",
    "file_name = str.split(dummy_data, \"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2418ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing sensitive column: CustomerBVN\n",
      "Hashing sensitive column: ID Number\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 15/15 [00:00<00:00, 22.35it/s, Completed]                       \n",
      "Render JSON: 100%|██████████| 1/1 [00:00<00:00, 32.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables statistics have successfully been copied into asset_custdata_variables.csv file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate report\n",
    "var_df = generate_profiling_report(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "747b0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = var_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fef6bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>n_distinct</th>\n",
       "      <th>p_distinct</th>\n",
       "      <th>is_unique</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>p_unique</th>\n",
       "      <th>type</th>\n",
       "      <th>hashable</th>\n",
       "      <th>value_counts_without_nan</th>\n",
       "      <th>value_counts_index_sorted</th>\n",
       "      <th>...</th>\n",
       "      <th>block_alias_counts</th>\n",
       "      <th>n_block_alias</th>\n",
       "      <th>block_alias_char_counts</th>\n",
       "      <th>script_counts</th>\n",
       "      <th>n_scripts</th>\n",
       "      <th>script_char_counts</th>\n",
       "      <th>category_alias_counts</th>\n",
       "      <th>n_category</th>\n",
       "      <th>category_alias_char_counts</th>\n",
       "      <th>word_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CustAID</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>{'bdd640fb-0667-4ad1-9c80-317fa3b1799d': 1, '1...</td>\n",
       "      <td>{'01d74256-3860-4ab6-96a4-02f23ae8cc93': 1, '0...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ASCII': 7776}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ASCII': {'-': 864, '4': 604, 'a': 487, '8': ...</td>\n",
       "      <td>{'Common': 5184, 'Latin': 2592}</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Common': {'-': 864, '4': 604, '8': 469, '9':...</td>\n",
       "      <td>{'Decimal Number': 4320, 'Lowercase Letter': 2...</td>\n",
       "      <td>3</td>\n",
       "      <td>{'Dash_Punctuation': {'-': 864}, 'Decimal_Numb...</td>\n",
       "      <td>{'bdd640fb-0667-4ad1-9c80-317fa3b1799d': 1, '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CustomerBVN</td>\n",
       "      <td>213</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>False</td>\n",
       "      <td>212</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>{'3973e022e93220f9212c18d0d0c543ae7c309e46640d...</td>\n",
       "      <td>{'02003e343641bc47361af297a6adcf04bb9a0d36f75c...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ASCII': 13824}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ASCII': {'4': 899, '8': 891, 'd': 890, '5': ...</td>\n",
       "      <td>{'Common': 8620, 'Latin': 5204}</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Common': {'4': 899, '8': 891, '5': 887, '3':...</td>\n",
       "      <td>{'Decimal Number': 8620, 'Lowercase Letter': 5...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Decimal_Number': {'4': 899, '8': 891, '5': 8...</td>\n",
       "      <td>{'3973e022e93220f9212c18d0d0c543ae7c309e46640d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID Number</td>\n",
       "      <td>189</td>\n",
       "      <td>0.875</td>\n",
       "      <td>False</td>\n",
       "      <td>164</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>{'a7937b64b8caa58f03721bb6bacf5c78cb235febe0e7...</td>\n",
       "      <td>{'0282d9b79f42c74c1550b20ff2dd16aafc3fe5d8ae9a...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ASCII': 13824}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ASCII': {'0': 967, '3': 910, 'c': 905, 'e': ...</td>\n",
       "      <td>{'Common': 8666, 'Latin': 5158}</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Common': {'0': 967, '3': 910, '7': 882, '8':...</td>\n",
       "      <td>{'Decimal Number': 8666, 'Lowercase Letter': 5...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Decimal_Number': {'0': 967, '3': 910, '7': 8...</td>\n",
       "      <td>{'a7937b64b8caa58f03721bb6bacf5c78cb235febe0e7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gender</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Female': 123, 'Male': 93}</td>\n",
       "      <td>{'Female': 123, 'Male': 93}</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ASCII': 1110}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ASCII': {'e': 339, 'a': 216, 'l': 216, 'F': ...</td>\n",
       "      <td>{'Latin': 1110}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Latin': {'e': 339, 'a': 216, 'l': 216, 'F': ...</td>\n",
       "      <td>{'Lowercase Letter': 894, 'Uppercase Letter': ...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Lowercase_Letter': {'e': 339, 'a': 216, 'l':...</td>\n",
       "      <td>{'female': 123, 'male': 93}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>187</td>\n",
       "      <td>0.865741</td>\n",
       "      <td>False</td>\n",
       "      <td>161</td>\n",
       "      <td>0.74537</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Consulting civil engineer': 4, 'Pilot, airli...</td>\n",
       "      <td>{'Academic librarian': 1, 'Accommodation manag...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ASCII': 4366}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ASCII': {'e': 459, 'r': 384, 'i': 375, 'a': ...</td>\n",
       "      <td>{'Latin': 4000, 'Common': 366}</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Latin': {'e': 459, 'r': 384, 'i': 375, 'a': ...</td>\n",
       "      <td>{'Lowercase Letter': 3774, 'Space Separator': ...</td>\n",
       "      <td>6</td>\n",
       "      <td>{'Lowercase_Letter': {'e': 459, 'r': 384, 'i':...</td>\n",
       "      <td>{'engineer': 25, 'manager': 18, 'officer': 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Account Officer</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Carol Colon': 1, 'Michael Sloan': 1, 'Kristi...</td>\n",
       "      <td>{'Aaron Carlson': 1, 'Alison Williams': 1, 'Am...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'ASCII': 2865}</td>\n",
       "      <td>1</td>\n",
       "      <td>{'ASCII': {'e': 260, 'a': 257, ' ': 228, 'n': ...</td>\n",
       "      <td>{'Latin': 2631, 'Common': 234}</td>\n",
       "      <td>2</td>\n",
       "      <td>{'Latin': {'e': 260, 'a': 257, 'n': 219, 'r': ...</td>\n",
       "      <td>{'Lowercase Letter': 2182, 'Uppercase Letter':...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Lowercase_Letter': {'e': 260, 'a': 257, 'n':...</td>\n",
       "      <td>{'michael': 8, 'matthew': 6, 'smith': 6, 'mr':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       column_name n_distinct p_distinct is_unique n_unique  p_unique  \\\n",
       "0          CustAID        216        1.0      True      216       1.0   \n",
       "1      CustomerBVN        213   0.986111     False      212  0.981481   \n",
       "2        ID Number        189      0.875     False      164  0.759259   \n",
       "3           Gender          2   0.009259     False        0       0.0   \n",
       "4       Occupation        187   0.865741     False      161   0.74537   \n",
       "5  Account Officer        216        1.0      True      216       1.0   \n",
       "\n",
       "          type hashable                           value_counts_without_nan  \\\n",
       "0  Categorical     True  {'bdd640fb-0667-4ad1-9c80-317fa3b1799d': 1, '1...   \n",
       "1  Categorical     True  {'3973e022e93220f9212c18d0d0c543ae7c309e46640d...   \n",
       "2  Categorical     True  {'a7937b64b8caa58f03721bb6bacf5c78cb235febe0e7...   \n",
       "3  Categorical     True                        {'Female': 123, 'Male': 93}   \n",
       "4  Categorical     True  {'Consulting civil engineer': 4, 'Pilot, airli...   \n",
       "5  Categorical     True  {'Carol Colon': 1, 'Michael Sloan': 1, 'Kristi...   \n",
       "\n",
       "                           value_counts_index_sorted  ... block_alias_counts  \\\n",
       "0  {'01d74256-3860-4ab6-96a4-02f23ae8cc93': 1, '0...  ...    {'ASCII': 7776}   \n",
       "1  {'02003e343641bc47361af297a6adcf04bb9a0d36f75c...  ...   {'ASCII': 13824}   \n",
       "2  {'0282d9b79f42c74c1550b20ff2dd16aafc3fe5d8ae9a...  ...   {'ASCII': 13824}   \n",
       "3                        {'Female': 123, 'Male': 93}  ...    {'ASCII': 1110}   \n",
       "4  {'Academic librarian': 1, 'Accommodation manag...  ...    {'ASCII': 4366}   \n",
       "5  {'Aaron Carlson': 1, 'Alison Williams': 1, 'Am...  ...    {'ASCII': 2865}   \n",
       "\n",
       "  n_block_alias                            block_alias_char_counts  \\\n",
       "0             1  {'ASCII': {'-': 864, '4': 604, 'a': 487, '8': ...   \n",
       "1             1  {'ASCII': {'4': 899, '8': 891, 'd': 890, '5': ...   \n",
       "2             1  {'ASCII': {'0': 967, '3': 910, 'c': 905, 'e': ...   \n",
       "3             1  {'ASCII': {'e': 339, 'a': 216, 'l': 216, 'F': ...   \n",
       "4             1  {'ASCII': {'e': 459, 'r': 384, 'i': 375, 'a': ...   \n",
       "5             1  {'ASCII': {'e': 260, 'a': 257, ' ': 228, 'n': ...   \n",
       "\n",
       "                     script_counts n_scripts  \\\n",
       "0  {'Common': 5184, 'Latin': 2592}         2   \n",
       "1  {'Common': 8620, 'Latin': 5204}         2   \n",
       "2  {'Common': 8666, 'Latin': 5158}         2   \n",
       "3                  {'Latin': 1110}         1   \n",
       "4   {'Latin': 4000, 'Common': 366}         2   \n",
       "5   {'Latin': 2631, 'Common': 234}         2   \n",
       "\n",
       "                                  script_char_counts  \\\n",
       "0  {'Common': {'-': 864, '4': 604, '8': 469, '9':...   \n",
       "1  {'Common': {'4': 899, '8': 891, '5': 887, '3':...   \n",
       "2  {'Common': {'0': 967, '3': 910, '7': 882, '8':...   \n",
       "3  {'Latin': {'e': 339, 'a': 216, 'l': 216, 'F': ...   \n",
       "4  {'Latin': {'e': 459, 'r': 384, 'i': 375, 'a': ...   \n",
       "5  {'Latin': {'e': 260, 'a': 257, 'n': 219, 'r': ...   \n",
       "\n",
       "                               category_alias_counts n_category  \\\n",
       "0  {'Decimal Number': 4320, 'Lowercase Letter': 2...          3   \n",
       "1  {'Decimal Number': 8620, 'Lowercase Letter': 5...          2   \n",
       "2  {'Decimal Number': 8666, 'Lowercase Letter': 5...          2   \n",
       "3  {'Lowercase Letter': 894, 'Uppercase Letter': ...          2   \n",
       "4  {'Lowercase Letter': 3774, 'Space Separator': ...          6   \n",
       "5  {'Lowercase Letter': 2182, 'Uppercase Letter':...          4   \n",
       "\n",
       "                          category_alias_char_counts  \\\n",
       "0  {'Dash_Punctuation': {'-': 864}, 'Decimal_Numb...   \n",
       "1  {'Decimal_Number': {'4': 899, '8': 891, '5': 8...   \n",
       "2  {'Decimal_Number': {'0': 967, '3': 910, '7': 8...   \n",
       "3  {'Lowercase_Letter': {'e': 339, 'a': 216, 'l':...   \n",
       "4  {'Lowercase_Letter': {'e': 459, 'r': 384, 'i':...   \n",
       "5  {'Lowercase_Letter': {'e': 260, 'a': 257, 'n':...   \n",
       "\n",
       "                                         word_counts  \n",
       "0  {'bdd640fb-0667-4ad1-9c80-317fa3b1799d': 1, '1...  \n",
       "1  {'3973e022e93220f9212c18d0d0c543ae7c309e46640d...  \n",
       "2  {'a7937b64b8caa58f03721bb6bacf5c78cb235febe0e7...  \n",
       "3                        {'female': 123, 'male': 93}  \n",
       "4  {'engineer': 25, 'manager': 18, 'officer': 15,...  \n",
       "5  {'michael': 8, 'matthew': 6, 'smith': 6, 'mr':...  \n",
       "\n",
       "[6 rows x 40 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "363a3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['column_name', 'n_distinct', 'p_distinct', 'is_unique', 'n_unique',\n",
       "       'p_unique', 'type', 'hashable', 'value_counts_without_nan',\n",
       "       'value_counts_index_sorted', 'ordering', 'n_missing', 'n', 'p_missing',\n",
       "       'count', 'memory_size', 'imbalance', 'first_rows', 'chi_squared',\n",
       "       'max_length', 'mean_length', 'median_length', 'min_length',\n",
       "       'length_histogram', 'histogram_length', 'n_characters_distinct',\n",
       "       'n_characters', 'character_counts', 'category_alias_values',\n",
       "       'block_alias_values', 'block_alias_counts', 'n_block_alias',\n",
       "       'block_alias_char_counts', 'script_counts', 'n_scripts',\n",
       "       'script_char_counts', 'category_alias_counts', 'n_category',\n",
       "       'category_alias_char_counts', 'word_counts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a252b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>n_distinct</th>\n",
       "      <th>p_distinct</th>\n",
       "      <th>is_unique</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>p_unique</th>\n",
       "      <th>type</th>\n",
       "      <th>n_missing</th>\n",
       "      <th>n</th>\n",
       "      <th>p_missing</th>\n",
       "      <th>count</th>\n",
       "      <th>max_length</th>\n",
       "      <th>mean_length</th>\n",
       "      <th>min_length</th>\n",
       "      <th>n_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CustAID</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>36</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CustomerBVN</td>\n",
       "      <td>213</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>False</td>\n",
       "      <td>212</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>64</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID Number</td>\n",
       "      <td>189</td>\n",
       "      <td>0.875</td>\n",
       "      <td>False</td>\n",
       "      <td>164</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>64</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gender</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>6</td>\n",
       "      <td>5.138889</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>187</td>\n",
       "      <td>0.865741</td>\n",
       "      <td>False</td>\n",
       "      <td>161</td>\n",
       "      <td>0.74537</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>57</td>\n",
       "      <td>20.212963</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Account Officer</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>25</td>\n",
       "      <td>13.263889</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       column_name n_distinct p_distinct is_unique n_unique  p_unique  \\\n",
       "0          CustAID        216        1.0      True      216       1.0   \n",
       "1      CustomerBVN        213   0.986111     False      212  0.981481   \n",
       "2        ID Number        189      0.875     False      164  0.759259   \n",
       "3           Gender          2   0.009259     False        0       0.0   \n",
       "4       Occupation        187   0.865741     False      161   0.74537   \n",
       "5  Account Officer        216        1.0      True      216       1.0   \n",
       "\n",
       "          type n_missing    n p_missing count max_length mean_length  \\\n",
       "0  Categorical         0  216       0.0   216         36        36.0   \n",
       "1  Categorical         0  216       0.0   216         64        64.0   \n",
       "2  Categorical         0  216       0.0   216         64        64.0   \n",
       "3  Categorical         0  216       0.0   216          6    5.138889   \n",
       "4  Categorical         0  216       0.0   216         57   20.212963   \n",
       "5  Categorical         0  216       0.0   216         25   13.263889   \n",
       "\n",
       "  min_length n_category  \n",
       "0         36          3  \n",
       "1         64          2  \n",
       "2         64          2  \n",
       "3          4          2  \n",
       "4          4          6  \n",
       "5          9          4  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_df[['column_name', 'n', 'n_missing', 'p_missing', 'n_distinct', 'p_distinct', 'is_unique', 'n_unique',\n",
    "       'p_unique', 'type', 'n_category', 'count','max_length', 'mean_length', 'min_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "print(f\"Generated profiling report and saved to {file_name}.html.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-profiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
